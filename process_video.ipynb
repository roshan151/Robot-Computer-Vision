{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1suTURbcmASH6CLNrsqG457ladQmp5x6V","authorship_tag":"ABX9TyP8Dhp2jMQc/WF3a15/REj0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RV8AC-rXhLOe","executionInfo":{"status":"ok","timestamp":1710020071083,"user_tz":360,"elapsed":21610,"user":{"displayName":"Roshan Tiwari","userId":"01488326468798660735"}},"outputId":"2a2c9b0f-4d75-47d8-93f4-dd9f8497fb47"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n","Requirement already satisfied: ultralytics in /root/.local/lib/python3.10/site-packages (8.1.25)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.8.0.76)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.1)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.31.0)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.11.4)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.0+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.16.0+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: thop>=0.1.1 in /root/.local/lib/python3.10/site-packages (from ultralytics) (0.1.1.post2209072238)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.49.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2023.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"]}],"source":["!pip install --user opencv-python\n","!pip install --user ultralytics"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","#!fusermount -u drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","#!google-drive-ocamlfuse drive"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xrn_z8ropyEN","executionInfo":{"status":"ok","timestamp":1710020114199,"user_tz":360,"elapsed":1945,"user":{"displayName":"Roshan Tiwari","userId":"01488326468798660735"}},"outputId":"abf71167-798f-4917-ad0d-34eda342cf5d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import glob\n","\n","import subprocess\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import IPython.display as ipd\n","from ultralytics import YOLO\n","from ultralytics.utils.plotting import Annotator\n","\n","filepath = '/content/gdrive/MyDrive/Computer_Vision'\n","\n","model = YOLO(\"yolov8m.pt\")"],"metadata":{"id":"pIaXcssfiSN1","executionInfo":{"status":"ok","timestamp":1710020125862,"user_tz":360,"elapsed":10033,"user":{"displayName":"Roshan Tiwari","userId":"01488326468798660735"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a69cd712-d005-437d-8c58-f4994c5ca1ad"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8m.pt to 'yolov8m.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 49.7M/49.7M [00:00<00:00, 161MB/s]\n"]}]},{"cell_type":"code","source":["file = 'beach_run.mp4'\n","ipd.Video(f'{filepath}/{file}', width = 500, embed = True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":304,"output_embedded_package_id":"1u0uuD1F5z1tJUPqRhLhJDlwH6UgFcN-e"},"id":"5_p0BK4nr2Xw","executionInfo":{"status":"ok","timestamp":1710020212370,"user_tz":360,"elapsed":7765,"user":{"displayName":"Roshan Tiwari","userId":"01488326468798660735"}},"outputId":"121d4a3d-a71b-4bfe-d3e9-2464985e27ca"},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["def detect_hand(result):\n","\n","  for r in result:\n","\n","    #annotator = Annotator(img)\n","\n","    boxes = r.boxes\n","    for box in boxes:\n","      class_name = model.names[int(box.cls)]\n","      #print(f'Class: {class_name}, Confidence: {box.conf} Box: {box.xyxy[0]}')\n","      #annotator.box_label(b, model.names[int(c)])\n","\n","      if class_name == 'hand':\n","        return True\n","\n","    return False\n","\n","  #img = annotator.result()\n","  #cv2.imshow('YOLO V8 Detection', img)\n","\n"],"metadata":{"id":"Vrktg3-kH-HJ","executionInfo":{"status":"ok","timestamp":1709870222954,"user_tz":360,"elapsed":128,"user":{"displayName":"Roshan Tiwari","userId":"01488326468798660735"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def calculate_distance( result, class_names, target_objects, object_size_dict):\n","\n","  boxes = result.boxes\n","\n","  for box in boxes:\n","      x1, y1, x2, y2 = box.xyxy[0]\n","      x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n","\n","      cls = int(box.cls)\n","\n","      if class_names[cls].lower() in target_objects:\n","          camera_width = x2 - x1\n","          distance = (real_width * frame_width) / camera_width\n","          #voice_notification(target_object)\n","\n","          obj_center_x = (x1 + x2) // 2\n","          obj_center_y = (y1 + y2) // 2\n","\n","          camera_middle_x = frame_width // 2\n","          camera_middle_y = frame_height // 2\n","\n","          vector_x = obj_center_x - camera_middle_x\n","          vector_y = obj_center_y - camera_middle_y\n","\n","          angle_deg = math.degrees(math.atan2(vector_y, vector_x))\n","          #direction = ''\n","          if angle_deg < 0:\n","              angle_deg += 360\n","\n","          if 0 <= angle_deg < 30:\n","              direction = \"3 o'clock\"\n","          elif 30 <= angle_deg < 60:\n","              direction = \"4 o'clock\"\n","          elif 60 <= angle_deg < 90:\n","              direction = \"5 o'clock\"\n","          elif 90 <= angle_deg < 120:\n","              direction = \"6 o'clock\"\n","          elif 120 <= angle_deg < 150:\n","              direction = \"7 o'clock\"\n","          elif 150 <= angle_deg < 180:\n","              direction = \"8 o'clock\"\n","          elif 180 <= angle_deg < 210:\n","              direction = \"9 o'clock\"\n","          elif 210 <= angle_deg < 240:\n","              direction = \"10 o'clock\"\n","          elif 240 <= angle_deg < 270:\n","              direction = \"11 o'clock\"\n","          elif 270 <= angle_deg < 300:\n","              direction = \"12 o'clock\"\n","          elif 300 <= angle_deg < 330:\n","              direction = \"1 o'clock\"\n","          elif 330 <= angle_deg < 360:\n","              direction = \"2 o'clock\"\n","          else:\n","              direction = \"Unknown Clock Position\"\n","\n","          cv2.putText(img, direction, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n","          cv2.putText(img, \"Distance: {:.2f} meters\".format(distance), (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","          cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n","\n","          if boxes is not None:\n","\n","              voice_notification(class_names[cls].lower(), direction, distance)\n"],"metadata":{"id":"rQg-3pWnl8JS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","import speech_recognition as sr\n","\n","def voice_command():\n","  recognizer = sr.Recognizer()\n","\n","  with sr.Microphone() as source:\n","      print(\"Waiting for voice command...\")\n","      recognizer.adjust_for_ambient_noise(source)\n","      audio = recognizer.listen(source)\n","\n","  target_object = \"\"\n","  real_width = 0.15\n","\n","  try:\n","      command = recognizer.recognize_google(audio, language=\"en-US\")\n","      print(\"Recognized command:\", command)\n","      last_word = get_last_word(command.lower())\n","      if last_word:\n","          print(\"Last word:\", last_word)\n","\n","      target_object = last_word.lower()\n","\n","      if target_object in object_dimensions:\n","          real_width = float(object_dimensions[target_object])\n","          print(real_width)\n","      else:\n","          print(f\"No length information found for {target_object}, using the default value of 0.15.\")\n","  except sr.UnknownValueError:\n","      print(\"Voice cannot be understood.\")\n","  except sr.RequestError as e:\n","      print(\"Voice recognition error; {0}\".format(e))\n","\n","  return target_object, real_width\n","\n","def voice_notification(obj_name, direction, distance):\n","  engine = pyttsx3.init()\n","  text = \"{} is at {}. It is {:.2f} meters away.\".format(obj_name, direction, distance)\n","  engine.say(text)\n","  engine.runAndWait()"],"metadata":{"id":"QxV-jAdLoG4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vid = cv2.VideoCapture(0)\n","fps = vid.get(cv2.CAP_PROP_FPS)\n","\n","frame_width = vid.get(cv2.CAP_PROP_FRAME_WIDTH)\n","frame_height = vid.get(cv2.CAP_PROP_FRAME_HEIGHT)\n","\n","print(f'Frames per second: {fps:0.2f}')\n","\n","ct = 0\n","ret = True\n","while ret == True:\n","\n","  ret, img = vid.read()\n","\n","  if ct % 10 ==0:\n","    cv2.imwrite('test_frame.jpg', img, [cv2.IMWRITE_JPEG_QUALITY, 100])\n","    result = model.predict('test_frame.jpg')\n","\n","    hand_check = detect_hand(result)\n","\n","    if hand_check == True:\n","      print(f'Detected hand in video at frame {ct}')\n","\n","    os.remove('test_frame.jpg')\n","\n","  ct += 1\n","\n","print(f'Number of frames {ct}')"],"metadata":{"id":"1Ee81cSTJvaq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfTQj0F2JN6J","executionInfo":{"status":"ok","timestamp":1709870269353,"user_tz":360,"elapsed":29929,"user":{"displayName":"Roshan Tiwari","userId":"01488326468798660735"}},"outputId":"6e9a7116-df0f-46a2-b3c3-4649d2934c16"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Frames per second: 19.92\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1189.4ms\n","Speed: 5.9ms preprocess, 1189.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 908.7ms\n","Speed: 2.5ms preprocess, 908.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 remote, 1 cell phone, 930.3ms\n","Speed: 2.3ms preprocess, 930.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 cell phone, 1020.4ms\n","Speed: 2.7ms preprocess, 1020.4ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1041.0ms\n","Speed: 2.9ms preprocess, 1041.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1030.2ms\n","Speed: 2.7ms preprocess, 1030.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1377.3ms\n","Speed: 2.8ms preprocess, 1377.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 cell phone, 1592.0ms\n","Speed: 3.3ms preprocess, 1592.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 cell phone, 1380.6ms\n","Speed: 4.1ms preprocess, 1380.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 cell phone, 953.6ms\n","Speed: 3.6ms preprocess, 953.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 remote, 1 cell phone, 942.8ms\n","Speed: 2.3ms preprocess, 942.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 cell phone, 1023.7ms\n","Speed: 2.7ms preprocess, 1023.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 cell phone, 1024.2ms\n","Speed: 2.8ms preprocess, 1024.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 remote, 1017.0ms\n","Speed: 2.8ms preprocess, 1017.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 cell phone, 949.8ms\n","Speed: 2.7ms preprocess, 949.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 donut, 1 cell phone, 941.0ms\n","Speed: 2.6ms preprocess, 941.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 cell phone, 939.1ms\n","Speed: 2.3ms preprocess, 939.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1116.4ms\n","Speed: 2.3ms preprocess, 1116.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 cell phone, 1484.3ms\n","Speed: 2.9ms preprocess, 1484.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 cell phone, 1582.4ms\n","Speed: 2.8ms preprocess, 1582.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","image 1/1 /content/test_frame.jpg: 384x640 1 person, 1 remote, 1027.6ms\n","Speed: 2.7ms preprocess, 1027.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","Number of frames 205\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"h_BmnemfJp6w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"_6WzXOS1Ji0t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"atrWXbGONU2n","executionInfo":{"status":"ok","timestamp":1709870301015,"user_tz":360,"elapsed":7,"user":{"displayName":"Roshan Tiwari","userId":"01488326468798660735"}},"outputId":"f6b7c257-1b23-4501-d4a2-74bbc02a9066"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pg1FQrk1Nj0P"},"execution_count":null,"outputs":[]}]}